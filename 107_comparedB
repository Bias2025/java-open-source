

## ============================================
## CONTEXT (Environmental Understanding)
## ============================================

CONTEXT:
  domain: "Financial Data Analysis & Reconciliation"
  purpose: "Automated comparison and variance analysis of dual financial datasets"
  scale_metrics:
    - expected_rows: "${DATASET_ROW_COUNT}"  # Placeholder: 1K-10M rows
    - expected_columns: "${DATASET_COLUMN_COUNT}"  # Placeholder: 10-500 columns
    - comparison_frequency: "${COMPARISON_CADENCE}"  # Daily/Weekly/Monthly/Quarterly
  
  data_sources:
    primary_dataset: "${PRIMARY_DATASET_PATH}"  # Placeholder for CSV/XLS file 1
    secondary_dataset: "${SECONDARY_DATASET_PATH}"  # Placeholder for CSV/XLS file 2
    
  constraints:
    - regulatory_compliance: ["SOX", "GDPR", "Basel III"]
    - precision_requirements: "6 decimal places for monetary values"
    - performance_threshold: "Sub-5 second analysis for datasets <100MB"
    
  integration_context:
    - upstream_systems: ["ERP", "GL", "Treasury Management"]
    - downstream_consumers: ["Risk Management", "Regulatory Reporting", "Executive Dashboard"]

## ============================================
## REQUIREMENTS (Explicit Specifications)
## ============================================

REQUIREMENTS:
  functional_requirements:
    data_ingestion:
      - "Parse and validate CSV/XLS/XLSX formats with automatic encoding detection"
      - "Handle multiple sheets with configurable sheet selection"
      - "Support for custom delimiters and quote characters"
      
    comparison_operations:
      - "Row-by-row comparison with configurable key columns"
      - "Column mapping with fuzzy name matching (Levenshtein distance ≤2)"
      - "Numeric tolerance thresholds (absolute and percentage)"
      - "Date/timestamp comparison with timezone awareness"
      - "Categorical data comparison with case sensitivity options"
      
    variance_detection:
      - "Identify missing rows in either dataset"
      - "Detect added rows unique to each dataset"
      - "Flag modified values exceeding tolerance thresholds"
      - "Calculate statistical significance of differences"
      
    output_generation:
      - "Comprehensive difference report with severity classification"
      - "Visual heat maps for variance concentration"
      - "Exportable reconciliation summary with drill-down capability"
      
  non_functional_requirements:
    performance:
      - latency: "P99 < 5 seconds for 1M row comparisons"
      - memory: "Maximum 2GB heap for 10M cell comparisons"
      
    accuracy:
      - precision: "Zero false negatives for material differences (>$1000)"
      - recall: "99.99% detection rate for all variances"
      
    auditability:
      - "Complete audit trail with timestamp and user attribution"
      - "Immutable comparison history with SHA-256 checksums"

## ============================================
## ARCHITECTURE (Design Decisions)
## ============================================

ARCHITECTURE:
  processing_pattern: "Stream-based chunked processing with parallel execution"
  
  comparison_strategy:
    primary_key_matching:
      - algorithm: "Hash-based join with fallback to sorted merge"
      - optimization: "Bloom filters for early mismatch detection"
      
    difference_engine:
      - numeric_comparison: "BigDecimal with configurable scale"
      - text_comparison: "Normalized Unicode with collation rules"
      - date_comparison: "Epoch milliseconds with timezone conversion"
      
  data_flow:
    1_ingestion_layer:
      - "Apache POI for Excel parsing"
      - "OpenCSV for CSV processing"
      - "Schema inference with type detection"
      
    2_normalization_layer:
      - "Column mapping and alignment"
      - "Data type standardization"
      - "Null handling strategies"
      
    3_comparison_layer:
      - "Parallel stream processing"
      - "Difference accumulation with categorization"
      - "Statistical analysis computation"
      
    4_presentation_layer:
      - "Report generation with Jasper/Thymeleaf"
      - "REST API for programmatic access"
      - "WebSocket for real-time updates"

## ============================================
## FRAMEWORK (Technical Implementation)
## ============================================

FRAMEWORK:
  core_stack:
    language: "Java 17"
    framework: "Spring Boot 3.2.x"
    build: "Maven/Gradle with multi-module structure"
    
  data_processing:
    parsing_libraries:
      - excel: "Apache POI 5.2.x"
      - csv: "OpenCSV 5.8"
      - json: "Jackson 2.15.x"
      
    comparison_engine:
      - "Spring Batch for large dataset processing"
      - "Project Reactor for reactive streams"
      - "Parallel Streams API for CPU-bound operations"
      
  persistence:
    database: "PostgreSQL 15+ with JSONB for flexible schema"
    caching: "Redis for comparison result caching"
    file_storage: "MinIO/S3 for dataset archival"
    
  visualization:
    - "Apache ECharts for interactive charts"
    - "D3.js for custom visualizations"
    - "AG-Grid for tabular difference display"
    
  monitoring:
    - "Micrometer metrics with Prometheus"
    - "ELK stack for centralized logging"
    - "Jaeger for distributed tracing"

## ============================================
## TESTING (Validation Strategy)
## ============================================

TESTING:
  test_coverage_targets:
    - unit: "95% line coverage with Mockito/JUnit 5"
    - integration: "TestContainers for PostgreSQL/Redis"
    - performance: "JMeter for load testing, JMH for microbenchmarks"
    
  test_scenarios:
    edge_cases:
      - "Empty datasets"
      - "Single row/column datasets"
      - "Completely identical datasets"
      - "Completely different datasets"
      - "Unicode and special characters"
      - "Scientific notation numbers"
      
    volume_testing:
      - "10M+ row comparisons"
      - "1000+ column datasets"
      - "Mixed data type columns"
      
    accuracy_testing:
      - "Floating point precision boundaries"
      - "Date boundary conditions (leap years, DST)"
      - "Null vs empty string differentiation"

## ============================================
## IMPLEMENTATION TEMPLATE
## ============================================

@Component
@Slf4j
public class FinancialDataComparator {
    
    private final DatasetParser datasetParser;
    private final ComparisonEngine comparisonEngine;
    private final ReportGenerator reportGenerator;
    
    /**
     * Main comparison orchestration method
     * Performance: O(n log n) for sorted comparison, O(n) for hash-based
     * Memory: O(k) where k is the number of differences
     */
    @Timed(value = "finance.comparison.duration")
    public ComparisonResult compareDatasets(
            @NonNull MultipartFile primaryDataset,
            @NonNull MultipartFile secondaryDataset,
            @Valid ComparisonConfig config) {
        
        // Parse datasets with automatic format detection
        Dataset primary = datasetParser.parse(primaryDataset, config);
        Dataset secondary = datasetParser.parse(secondaryDataset, config);
        
        // Perform intelligent column mapping
        ColumnMapping mapping = comparisonEngine.mapColumns(primary, secondary);
        
        // Execute comparison with configured strategies
        DifferenceSet differences = comparisonEngine.compare(
            primary, 
            secondary, 
            mapping, 
            config
        );
        
        // Generate comprehensive report
        return reportGenerator.generate(differences, config.getOutputFormat());
    }
}

## ============================================
## USAGE INSTRUCTIONS
## ============================================

USAGE:
  1. Replace placeholders:
     - ${PRIMARY_DATASET_PATH} → actual file path/upload
     - ${SECONDARY_DATASET_PATH} → actual file path/upload
     - ${DATASET_ROW_COUNT} → expected row count
     - ${DATASET_COLUMN_COUNT} → expected column count
     
  2. Configure comparison parameters:
     - tolerance_thresholds: {numeric: 0.01, percentage: 0.1%}
     - key_columns: ["account_id", "transaction_date"]
     - ignore_columns: ["last_modified", "system_timestamp"]
     
  3. Execute comparison:
     POST /api/v1/finance/compare
     Content-Type: multipart/form-data
     
  4. Retrieve results:
     GET /api/v1/finance/comparison/{id}/report
     Accept: application/json, application/pdf, application/vnd.ms-excel

## ============================================
## CURL EXAMPLES
## ============================================

# Upload and compare two datasets
curl -X POST https://api.example.com/v1/finance/compare \
  -H "Authorization: Bearer ${JWT_TOKEN}" \
  -F "primary=@dataset1.csv" \
  -F "secondary=@dataset2.xlsx" \
  -F 'config={"keyColumns":["account_id"],"tolerance":0.01}'

# Retrieve comparison report
curl -X GET https://api.example.com/v1/finance/comparison/abc123/report \
  -H "Authorization: Bearer ${JWT_TOKEN}" \
  -H "Accept: application/pdf" \
  -o comparison_report.pdf
